{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Function to read the YAML config file\n",
    "def read_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error reading YAML file: {exc}\")\n",
    "            return None\n",
    "\n",
    "config = read_yaml('./config.yaml')\n",
    "\n",
    "dataframes = {}\n",
    "if config and 'sheets' in config:\n",
    "    sheets_list = config['sheets']\n",
    "    for sheet in sheets_list:\n",
    "        dataframes[sheet] = pd.read_excel(\n",
    "            './data/raw_data-SA Exercise.xlsx', sheet_name=sheet, dtype={'EF ID CO2': 'Int64', 'EF CO2': 'Int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mock SAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sap_df = pd.DataFrame({\n",
    "    'SAP ID': [2, 5, 7, 27, 81, 45, 8, 12, 6, 24],\n",
    "    'Supplier Name': ['Glassy Glass inc.', 'Glassy Glass inc.', 'Spice girls inc.','Spice girls inc.', 'Spice girls inc.', 'Spice girls inc.', 'Ship happens inc.', 'Ship happens inc.', 'Dumpster Divers inc.', 'Dumpster Divers inc.']\n",
    "})\n",
    "\n",
    "\n",
    "for sheet_name, df in dataframes.items():\n",
    "    updated_df = df.merge(sap_df[['SAP ID', 'Supplier Name']], on='SAP ID', how='left')\n",
    "    dataframes[sheet_name] = updated_df\n",
    "    print(updated_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate schema against config\n",
    "**Required**: Activity Label | Activity Unit\t| Scope\t| Quantity | GHG Category\n",
    "\n",
    "**Optional**: Global Region\t| Country | EF CO2 | EF ID CO2\t| Taxonomy\t| Region | CO2AI Taxonomy\t\n",
    "\n",
    "**Custom Fields**: GBU\t| Very Custom\tCategory\t| SAP ID\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, DataFrameSchema\n",
    "\n",
    "# Function to create a pandera schema from the YAML config\n",
    "def create_schema_from_config(config):\n",
    "    schema_dict = {}\n",
    "    for field, dtype in config['schema']['required_fields'].items():\n",
    "        schema_dict[field] = Column(getattr(pa, dtype.capitalize()), nullable=False, coerce=True)\n",
    "    for field, dtype in config['schema']['optional_fields'].items():\n",
    "        schema_dict[field] = Column(getattr(pa, dtype.capitalize()), nullable=True, required=False)\n",
    "    for field, dtype in config['schema']['custom_fields'].items():\n",
    "        schema_dict[field] = Column(getattr(pa, dtype.capitalize()), nullable=True, required=False)\n",
    "    schema = DataFrameSchema(schema_dict, strict=True)\n",
    "    return schema\n",
    "\n",
    "# Get Schema\n",
    "schema = create_schema_from_config(config)\n",
    "\n",
    "# Validate the DataFrame\n",
    "for sheet_name, df in dataframes.items():\n",
    "    try:\n",
    "        dataframes[sheet_name] = schema.validate(df)\n",
    "        print(f\"{sheet_name} sheet is valid.\")\n",
    "    except pa.errors.SchemaError as e:\n",
    "        print(f\"{sheet_name} sheet is invalid.\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet_name, df in dataframes.items():\n",
    "  print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check required fields for allowed values\n",
    "GHG Categories:\n",
    "\n",
    "**Scope 1**\n",
    "\n",
    "- Direct emissions\n",
    "\n",
    "**Scope 2**\n",
    "\n",
    "- Energy and Electricity Consumption\n",
    "\n",
    "**Scope 3**\n",
    "\n",
    "- Purchased goods and services\n",
    "- Capital goods\n",
    "- Fuel and energy related activities\n",
    "- Upstream transportation and distribution\n",
    "- Waste generated in operations\n",
    "- Business travel\n",
    "- Employee commuting\n",
    "- Upstream leased assets\n",
    "- Downstream transportation and distribution\n",
    "- Processing of sold products\n",
    "- Use of sold products\n",
    "- End of life treatment of sold products\n",
    "- Downstream leased assets\n",
    "- Franchises\n",
    "- Investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to ensure Scope values are 1, 2, or 3\n",
    "\n",
    "valid_scope_values = [1, 2, 3]\n",
    "\n",
    "for sheet_name, df in dataframes.items():\n",
    "  df[\"is_valid_scope\"] = df[\"Scope\"].isin(valid_scope_values)\n",
    "\n",
    "  # Filter out invalid rows\n",
    "  invalid_scope_rows = df[~df[\"is_valid_scope\"]]\n",
    "  if not invalid_scope_rows.empty:\n",
    "      print(\"Scope Rows with invalid scope values:\")\n",
    "      print(invalid_scope_rows)\n",
    "  else:\n",
    "      print(\"All Scope Rows have valid scope values\")\n",
    "\n",
    "  df.drop(columns=[\"is_valid_scope\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to ensure scope value has valid corresponding category\n",
    "\n",
    "allowed_ghg_categories = {1: [\"Direct Emissions\"], 2: [\"Energy and Electricity Consumption\"], 3: [\"Purchased goods and services\", \"Capital goods\", \"Fuel and energy related activites\", \"Upstream transportation and distribution\",\n",
    "                                                                                                                          \"Waste generated in operations\", \"Business travel\", \"Employee commuting\", \"Upstream leased assets\", \"Downstream transportation and distribution\", \"Processing of sold products\", \"Use of sold products\", \"End of life treatment of sold products\", \"Downstream leased assets\", \"Franchises\", \"Investments\"]}\n",
    "def ghg_category_check(row):\n",
    "    scope = row[\"Scope\"]\n",
    "    ghg_category = row[\"GHG Category\"]\n",
    "    valid_categories = allowed_ghg_categories.get(scope)\n",
    "    return ghg_category in valid_categories\n",
    "\n",
    "# Apply the custom check function\n",
    "for sheet_name, df in dataframes.items():\n",
    "    df[\"valid\"] = df.apply(ghg_category_check, axis=1)\n",
    "\n",
    "    # Filter out invalid rows\n",
    "    invalid_rows = df[~df[\"valid\"]]\n",
    "    if not invalid_rows.empty:\n",
    "        print(\"Custom check validation errors:\")\n",
    "        print(invalid_rows)\n",
    "    else:\n",
    "        print(\"All rows passed the custom check validation\")\n",
    "\n",
    "    # Drop the auxiliary 'valid' column\n",
    "    df.drop(columns=[\"valid\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add default taxonomy values based on config\n",
    "\n",
    "# default_taxonomies = config['default_taxonomies']\n",
    "# \n",
    "# for sheet_name, df in dataframes.items():\n",
    "#   df['Category'] = df['Category'].str.title()\n",
    "#   df['Taxonomy'] = df['Category'].map(default_taxonomies)\n",
    "#   dataframes[sheet_name] = df\n",
    "#   print(dataframes[sheet_name].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for valid units\n",
    "If an invalid dimension exists then we need to update it to the valid dimension and update the quantity\n",
    "\n",
    "Dimension\tUnit\n",
    "- CO₂ Emissions : kgCO2eq\n",
    "- Currency : usd\n",
    "- Weight: kg\n",
    "- Distance : km\n",
    "- Surface: m2\n",
    "- Volume: m3\n",
    "- Energy: kWh\n",
    "- Time: day\n",
    "- Person: individual\n",
    "- Vehicle: vehicle\n",
    "- Dimensionless: dimensionless\n",
    "- Item: item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_factors = {\n",
    "    \"g\": {\"to\": \"kg\", \"factor\": 0.001},        # 1 gram is 0.001 kg\n",
    "    \"lbs\": {\"to\": \"kg\", \"factor\": 0.453592},   # 1 lb is approximately 0.453592 kg\n",
    "    \"miles\": {\"to\": \"km\", \"factor\": 1.60934}   # 1 mile is approximately 1.60934 km\n",
    "}\n",
    "\n",
    "def update_activity_unit(row):\n",
    "    activity_unit = row[\"Activity Unit\"]\n",
    "    quantity = row[\"Quantity\"]\n",
    "    \n",
    "    if activity_unit in conversion_factors:\n",
    "        row[\"Activity Unit\"] = conversion_factors[activity_unit][\"to\"]\n",
    "        row[\"Quantity\"] = quantity * conversion_factors[activity_unit][\"factor\"]\n",
    "    \n",
    "    return row\n",
    "\n",
    "for sheet_name, df in dataframes.items():\n",
    "    dataframes[sheet_name] = df.apply(update_activity_unit, axis=1)\n",
    "    print(dataframes[sheet_name].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check country names\n",
    "Ensure country names are a part of valid list -> https://co2-ai.gitbook.io/delivery-documentation/appendix/allowed-values/locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "country_mapping = {\n",
    "    \"España\": \"Spain\",\n",
    "    \"Russia\": \"Russia\"\n",
    "}\n",
    "\n",
    "def normalize_country_name(country_name):\n",
    "    try:\n",
    "        country = pycountry.countries.lookup(country_name)\n",
    "        return country.name.lower()\n",
    "    except LookupError:\n",
    "        if country_name in country_mapping:\n",
    "            return country_mapping[country_name].lower()\n",
    "        elif country_name not in country_mapping and country_name not in country_mapping.values():\n",
    "            raise ValueError(f\"Country name '{country_name}' is not in the mapping. Please add it to the mapping.\")\n",
    "\n",
    "for sheet_name, df in dataframes.items():\n",
    "    try:\n",
    "        df['Country'] = df['Country'].apply(normalize_country_name)\n",
    "        dataframes[sheet_name] = df\n",
    "        print(dataframes[sheet_name].to_string())\n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sheets into one\n",
    "Add data source column and drop null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for sheet_name, df in dataframes.items():\n",
    "    df[\"Data Source\"] = sheet_name\n",
    "    df_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df_cleaned = combined_df.dropna(axis=1, how='all')\n",
    "\n",
    "print(df_cleaned.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop fields based on config\n",
    "  - If `use_preset_ef` is set to `false` then we will not use the `EF CO2 || EF ID CO2` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_preset_ef'] == False:\n",
    "  preset_ef_cols = ['EF CO2', 'EF ID CO2', \"CO2AI Taxonomy\"]\n",
    "  cols_to_drop = [col for col in preset_ef_cols if col in df.columns]\n",
    "  if cols_to_drop:\n",
    "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "elif config['use_preset_ef'] == True:\n",
    "  df_cleaned['EF ID CO2'] = df_cleaned['EF ID CO2'].astype('Int64')\n",
    "  df_to_validate = df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Validation\n",
    "And create new excel spreadsheet that will be uploaded to platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_validated_df = schema.validate(df_to_validate)\n",
    "    print(\"Final spreadsheet is valid.\")\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(\"Spreadsheet is invalid.\")\n",
    "    print(df.dtypes.to_string())\n",
    "    print(e)\n",
    "\n",
    "print(final_validated_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as excel\n",
    "combined_excel_path = \"./data/combined_cleansed-SA Exercise.xlsx\"\n",
    "\n",
    "final_validated_df.to_excel(combined_excel_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
